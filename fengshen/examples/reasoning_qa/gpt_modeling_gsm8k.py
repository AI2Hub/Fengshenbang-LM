# ====================================================
#   Copyright (C) 2022  All rights reserved.
#
#   Author        : Xinyu Zhu
#   Email         : zhuxy21@mails.tsinghua.edu.cn
#   File Name     : gpt_modeling_gsm8k.py
#   Last Modified : 2022-07-15 15:48
#   Describe      : 
#
# ====================================================
import os
import jsonlines
import itertools
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer
from gpt_modeling_base import GPT2BaseModel
from math_data_model import extract_answer, ANS_RE, INVALID_ANS
#  from calculator import sample
from calculator import batch_calculator_sample as sample
from pysnooper import snoop
from torchsnooper import snoop as tsnoop


class GPT2ModelForGSM8K(GPT2BaseModel):
    """
    initiates a PyTorch Lightning GPT2 base model for training on GSM8K, defines training and evaluation steps
    """
    @staticmethod
    def add_model_specific_args(parent_parser):
        """
        Add GPT specific args
        Returns:
            parent_parser
        """
        parser = parent_parser.add_argument_group('GPT2ModelForGSM8K')
        parser.add_argument('--loss_on_prefix', action="store_true", default=False, help="Compute loss on question tokens")
        parser.add_argument('--prompt', action="store_true", default=False, help="Add chain of thought prompt before test question")
        parser.add_argument('--generator', action="store_true", default=False, help="Perform as a generator to generate solutions for training verifier")
        parser.add_argument('--temp', default=1.0, type=float, help="Temperature of generator when sampling")
        parser.add_argument('--num_sample', default=100, type=int, help="How many solutions to sample for each question")
        parser.add_argument('--sample_len', default=160, type=int, help="Maximum sample len")
        parser.add_argument('--comment', default=None, type=str, help="Comment for creating save dir")
        parser.add_argument('--mcts_finetune', action="store_true", default=False, help="Use samples generated by MCTS for likelihood and unlikelihood training")
        #  parser.add_argument('--text', action="store_true", default=False)
        #  parser.add_argument('--expr', action="store_true", default=False)
        #  parser.add_argument('--reason', action="store_true", default=False)
        #  parser.add_argument('--oracle', action="store_true", default=False)

        return parent_parser

    def __init__(self, args, model=None, tokenizer=None):
        super().__init__(args, model, tokenizer)

    #  def forward(self, input_ids, attention_mask, labels=None):
    #      """ forward step """
    #      # TODO 修改math_data_model 使这里计算loss不需要自己用nn.CrossEntropy, 就可以复用base的forward
    #      output = self.model(
    #          input_ids,
    #          attention_mask=attention_mask,
    #          labels=labels,
    #      )
    #      # Flatten the tokens
    #      loss_fct = CrossEntropyLoss()
    #      loss = loss_fct(output.logits.view(-1, output.logits.size(-1)), labels.view(-1))
    #
    #      return loss, output.logits

    def get_inputs(self, batch):
        inputs = {
            'input_ids': batch['input_ids'],
            'attention_mask': batch['attention_mask'],
            'labels': batch['labels'],
        }
        if self.hparams.mcts_finetune and 'is_correct' in batch:
            inputs['is_correct'] = batch['is_correct']
            inputs['verifier_score'] = batch['verifier_score']

        return inputs

    def training_step(self, batch, batch_idx):
        """ training step """
        inputs = self.get_inputs(batch)
        input_ids = inputs["input_ids"]
        batch_size = input_ids.size(0)
        seq_len = input_ids.size(1)
        self._consumed_samples += batch_size * max(self.trainer.gpus, 1)  # batch size * data parallel size
        # GPT的labels就是input_ids往右偏移一位，以及pad部分被设为了-100，没法decode，所以下面要把labels==-100还原
        labels = inputs["labels"]
        self._consumed_tokens += len(labels.flatten()) * max(self.trainer.gpus, 1)

        #  loss, logits = self(**inputs)
        loss, logits = self(input_ids, inputs['attention_mask'], labels)
        ts_logger = self.logger.experiment

        if self.hparams.mcts_finetune:
            logits = logits[..., :-1, :].contiguous()
            labels = labels[..., 1:].contiguous()
            seq_len -= 1

            is_correct = batch['is_correct']
            verifier_score = batch['verifier_score']
            prob = F.log_softmax(logits, dim=-1)

            # START version 0.2.1 only weighted likelihood
            loss_fct = nn.CrossEntropyLoss(reduction='none')
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
            loss = loss.view(batch_size, seq_len)
            loss = torch.mean(loss, dim=-1)
            loss *= (1 + verifier_score)
            mle_loss = loss.mean()
            self.log("mle_loss_step", mle_loss, prog_bar=True, logger=True, on_step=True, batch_size=batch_size)
            ts_logger.add_scalar("mle_loss_vs_samples", mle_loss.item(), self._consumed_samples)
            loss = torch.mean(loss)
            # END

            # START version 0.2 weighted likelihood and unlikelihood training
            #  loss_fct = nn.CrossEntropyLoss(reduction='none')
            #  loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
            #  loss = loss.view(batch_size, seq_len)
            #  loss = torch.mean(loss, dim=-1)
            #  if is_correct.any():
            #      mle_loss = loss[is_correct].mean()
            #      self.log("mle_loss_step", mle_loss, prog_bar=True, logger=True, on_step=True, batch_size=batch_size)
            #      ts_logger.add_scalar("mle_loss_vs_samples", mle_loss.item(), self._consumed_samples)
            #  if not is_correct.all():
            #      ul_loss = -loss[is_correct.eq(False)].mean()
            #      self.log("ul_loss_step", ul_loss, prog_bar=True, logger=True, on_step=True, batch_size=batch_size)
            #      ts_logger.add_scalar("ul_loss_vs_samples", ul_loss.item(), self._consumed_samples)
            #  loss *= torch.where(is_correct, 1., -0.1)
            #  loss *= (1 + verifier_score)
            #  loss = torch.mean(loss)
            # END


            #  START version 0.1 weighted likelihood and unlikelihood training
            #  if is_correct.any():
            #      mle_logits = logits[is_correct]  # (bs_mle, seq_len, vocab_size)
            #      mle_labels = labels[is_correct]  # (bs_mle, seq_len)
            #      mle_verifier_score = verifier_score[is_correct]  # (bs_mle, )
            #
            #      #  loss_fct = nn.CrossEntropyLoss(reduction='none')
            #      loss_fct = nn.CrossEntropyLoss()
            #      mle_loss = loss_fct(mle_logits.view(-1, mle_logits.size(-1)), mle_labels.view(-1))
            #      #  mle_loss = F.nll_loss(mle_prob.view(-1, mle_prob.size(-1)), mle_labels.view(-1), reduction='none')
            #      #  mle_loss = mle_loss.view(-1, seq_len)
            #      #  assert mle_loss.size() == (sum(is_correct), seq_len)
            #      #  mle_loss = torch.mean(mle_loss, dim=-1)
            #      mle_loss = (1 + mle_verifier_score) * mle_loss
            #      #  mle_loss = torch.mean(mle_loss)
            #
            #      self.log("mle_loss_step", mle_loss, prog_bar=True, logger=True, on_step=True, batch_size=batch_size)
            #      ts_logger.add_scalar("mle_loss_vs_samples", mle_loss.item(), self._consumed_samples)
            #
            #      if torch.isnan(mle_loss):
            #          mle_prob = prob[is_correct]
            #          mle_target_prob = torch.gather(mle_prob, -1, mle_labels.unsqueeze(-1)).squeeze(-1)  # (bs_ul, seq_len)
            #          print('mle_loss', mle_loss)
            #          #  print('mle_logits', mle_logits.data)
            #          print('mle_target_prob', mle_target_prob.exp().data)
            #          print('mle_labels', mle_labels.data)
            #  else:
            #      mle_loss = None
            #
            #  if not is_correct.all():
            #      ul_labels = labels[is_correct.eq(False)]  # (bs_ul, seq_len)
            #      ul_labels = torch.where(ul_labels == -100, self.tokenizer.convert_tokens_to_ids('[VERIFIER]'), ul_labels)
            #      ul_prob = prob[is_correct.eq(False)]  # (bs_ul, seq_len, vocab_size)
            #      ul_target_prob = torch.gather(ul_prob, -1, ul_labels.unsqueeze(-1)).squeeze(-1)  # (bs_ul, seq_len)
            #      assert ul_target_prob.size() == (sum(is_correct.eq(False)), seq_len)
            #      ul_verifier_score = verifier_score[is_correct.eq(False)]
            #
            #      clamp_min = 1e-5
            #      ul_loss = -torch.log(torch.clamp(1.0 - ul_target_prob.exp(), min=clamp_min))  # (bs_ul, seq_len)
            #
            #      assert ul_loss.size() == (sum(is_correct.eq(False)), seq_len)
            #      ul_loss *= (ul_labels != self.tokenizer.convert_tokens_to_ids('[VERIFIER]')).int()
            #      ul_loss = torch.mean(ul_loss, dim=-1)
            #      ul_loss = (1 + ul_verifier_score) * ul_loss
            #      ul_loss = torch.mean(ul_loss)
            #
            #      self.log("ul_loss_step", ul_loss, prog_bar=True, logger=True, on_step=True, batch_size=batch_size)
            #      ts_logger.add_scalar("ul_loss_vs_samples", ul_loss.item(), self._consumed_samples)
            #
            #      if torch.isnan(ul_loss):
            #          print('ul_loss', ul_loss)
            #          print('ul_target_prob', ul_target_prob.exp().data)
            #          print('ul_labels', ul_labels.data)
            #  else:
            #      ul_loss = None
            #
            #  loss = 0
            #  if mle_loss:
            #      loss += mle_loss
            #  if ul_loss:
            #      loss += ul_loss

            #  END version 0.1 weighted likelihood and unlikelihood training

        if self.hparams.show_training_ex > -1 and batch_idx % self.hparams.show_training_ex == 0:
            if self.hparams.mcts_finetune:
                self.show_training_example(input_ids=input_ids[0][:-1], labels=labels[0], logits=logits[0])
            else:
                self.show_training_example(input_ids=input_ids[0], labels=labels[0], logits=logits[0])

        self.log("train_loss_step", loss, prog_bar=True, logger=True, on_step=True, batch_size=batch_size)
        ts_logger.add_scalar("train_loss_vs_samples", loss.item(), self._consumed_samples)
        ts_logger.add_scalar("train_loss_vs_tokens", loss.item(), self._consumed_tokens)

        # Do custom things for your task
        custom_output_dict = self.custom_training_step(batch, batch_idx, logits)

        output_dict = {"loss": loss}
        if custom_output_dict is not None:
            output_dict.update(custom_output_dict)
        #  current_step = self.trainer.lr_schedulers[0]['scheduler']._step_count

        return output_dict

    def custom_validation_step(self, batch, batch_idx, logits):
        batch_size = batch["input_ids"].size(0)
        return {'num_total': batch_size, 'question': batch['question'], 'answer': batch['answer']}

    def generate_step(self, batch, batch_idx):
        question = batch['question']
        answer = batch['answer']
        question_id = batch['question_id']
        solutions = []

        pred, generated_token_ids = sample(self.model, question, self.tokenizer, self.device, sample_len=self.hparams.sample_len, 
                                        do_sample=True, temperature=self.hparams.temp)
        generated_solutions = self.tokenizer.batch_decode(generated_token_ids)

        generator_file = "generator_solution.jsonl" + str(self.global_rank)
        with jsonlines.open(os.path.join(self.hparams.save_dir, self.hparams.timestamp + '-' + generator_file), 'a') as f:
            if batch_idx == 0:
                f.write({"new_iteration": "1", "solution": "[ANS] 0<|endoftext|>"})
            for idx, solution in enumerate(generated_solutions):
                pred_answer = extract_answer(solution)
                gt_answer = extract_answer(answer[idx])
                assert gt_answer != INVALID_ANS
                solutions.append(solution)
                f.write({"question": question[idx], "ground_truth": answer[idx],
                    "solution": solution, "is_correct": pred_answer == gt_answer, "question_id": question_id[idx]})
                if pred_answer == gt_answer:
                    print('*' * 50)
                    print("question: ", question[idx])
                    print("predicted answer: ", solution)
                    print("gold answer: ", answer[idx])
                    print('*' * 50)

        #  return {'question': question, 'answer': answer, 'solutions': solutions, 'is_correct': is_correct}

    def predict_step(self, batch, batch_idx):
        """ batch calculator predict step """
        if self.hparams.generator:
            self.generate_step(batch, batch_idx)
            return
        question = batch['question']
        answer = batch['answer']
        question_id = batch['question_id']
        solutions = []
        is_correct = []

        if batch_idx % 100 == 0:
            print(f"Current predict example index: {batch_idx}.")

        pred, generated_token_ids = sample(self.model, question, self.tokenizer, self.device, sample_len=self.hparams.sample_len)
        generated_solutions = self.tokenizer.batch_decode(generated_token_ids)

        solution_file = "model_solution.jsonl" + str(self.global_rank)
        if self.hparams.prompt:
            solution_file = "prompt_" + solution_file
        with jsonlines.open(os.path.join(self.hparams.save_dir, self.hparams.timestamp + '-' + solution_file), 'a') as f:
            for idx, solution in enumerate(generated_solutions):
                pred_answer = extract_answer(solution)
                gt_answer = extract_answer(answer[idx])
                assert gt_answer != INVALID_ANS
                is_correct.append(int(pred_answer == gt_answer))
                solutions.append(solution)
                f.write({"question": question[idx], "ground_truth": answer[idx],
                    "solution": solution, "is_correct": pred_answer == gt_answer, "question_id": question_id[idx]})
                if pred_answer == gt_answer:
                    print('*' * 50)
                    print("question: ", question[idx])
                    print("predicted answer: ", solution)
                    print("gold answer: ", answer[idx])
                    print('*' * 50)

        return {'question': question, 'answer': answer, 'solutions': solutions, 'is_correct': is_correct}

    #  def predict_step_single_calculator(self, batch, batch_idx):
    #      """ single calculator predict step """
    #      inputs = self.get_inputs(batch)
    #      batch_size = inputs["input_ids"].size(0)
    #      #  assert batch_size == 1
    #      num_correct = 0
    #      solutions = []
    #      is_correct = []
    #
    #      if batch_idx % 100 == 0:
    #          print(f"Current predict example index: {batch_idx}.")
    #
    #      for idx, qn in enumerate(batch['question']):
    #          #  len_q = len(qn)
    #          pred, generated_token_ids = sample(self.model, qn, self.tokenizer, self.device, sample_len=160)
    #          # TODO 不知道为什么模型会在=后面输出<endoftext>
    #          #  if generated_token_ids.count(self.tokenizer.eos_token_id) > 1:
    #          #      print("---------more than one endoftext-----------")
    #          #      print(self.tokenizer.decode(generated_token_ids))
    #          #      print("-------------------------------------------")
    #          solution = self.tokenizer.decode(generated_token_ids)
    #          pred_answer = extract_answer(solution)
    #          gt_answer = extract_answer(batch['answer'][idx])
    #          assert gt_answer != INVALID_ANS
    #          is_correct.append(int(pred_answer == gt_answer))
    #          solutions.append(solution)
    #          #  num_correct += int(pred_answer == gt_answer)
    #          #  solution_file = "model_solution.jsonl"
    #          #  if self.hparams.prompt:
    #          #      solution_file = "prompt_" + solution_file
    #          #  with jsonlines.open(os.path.join(self.hparams.save_dir, solution_file), 'a', flush=True) as f:
    #          #      f.write({"question": qn, "ground_truth": batch['answer'][idx],
    #          #              "solution": solution, "is_correct": pred_answer == gt_answer})
    #          if pred_answer == gt_answer:
    #              print('*' * 50)
    #              print("question: ", qn)
    #              print("predicted answer: ", solution)
    #              print("gold answer: ", batch['answer'][idx])
    #              print('*' * 50)
    #
    #      return {'question': batch['question'], 'answer': batch['answer'], 'solutions': solutions,
    #              'is_correct': is_correct}


    #  def predict_step_batch_without_calculator(self, batch, batch_idx):
    #      """ batch without calculator predict step"""
    #      num_correct = 0
    #      solutions = []
    #      is_correct = []
    #
    #      self.tokenizer.padding_side = "left"
    #      inputs_encoding = self.tokenizer(
    #          batch['question'],
    #          return_attention_mask=True,
    #          return_tensors="pt",
    #          add_special_tokens=False,
    #          padding=True,
    #      )
    #      self.tokenizer.padding_side = "right"
    #      inputs_encoding = inputs_encoding.to(self.device)
    #      generated_ids, _ = self.model.generate(
    #          **inputs_encoding,
    #          max_length=160,
    #          pad_token_id=self.tokenizer.pad_token_id,
    #      )
    #      # if num_return_sequences>1, then batch_decode returns batch_size * num_return_sequences results
    #      predicted_tokens = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)
    #      solution_file = "model_solution.jsonl" + str(self.global_rank)
    #      if self.hparams.prompt:
    #          solution_file = "prompt_" + solution_file
    #      with jsonlines.open(os.path.join(self.hparams.save_dir, self.hparams.timestamp + '-' + solution_file), 'a') as f:
    #          for q, solution, gt in zip(batch['question'], predicted_tokens, batch['answer']):
    #              pred_answer = extract_answer(solution)
    #              gt_answer = extract_answer(gt)
    #              is_correct.append(int(pred_answer == gt_answer))
    #              solutions.append(solution)
    #              f.write({"question": q, "ground_truth": gt,
    #                      "solution": solution, "is_correct": pred_answer == gt_answer})
    #              if pred_answer == gt_answer:
    #                  print('*' * 50)
    #                  print("predicted answer: ", solution)
    #                  print("gold answer: ", gt)
    #                  print('*' * 50)
    #
    #      return {'question': batch['question'], 'answer': batch['answer'], 'solutions': solutions,
    #              'is_correct': is_correct}
